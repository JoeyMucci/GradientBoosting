# History of Gradient Boosting
Before gradient boosting came Adaboost, which used the "boosting" principle of each model iterating on the previous model. It gives weights to the mistakes of the previous model in order to fix them for the next iteration. Adaboost worked well but there was still skepticism surrounding it in the machine learning community. Developed by Jerome Friedman in 1999, Gradient Boosting Machine (GBM) generalized boosting methods to be much more widely applicable, and also had a much more rigorous mathematical basis. Although it was not immediately popular, GBM gradually gained merit as data scientists began to see its capabilities. Kaggle actually served as an advertiser for gradient boosting, as a majority of competitions were won by programmers using the XGBoost library for gradient boosting. With its potential shown, GBM soon became an essential unit of the machine learning toolkit. 
# Gradient Boosting Method
Gradient boosting is versatile, however all of its applications follow the same formula. Like all boosting methods, the goal is to use multiple weak models to create one strong model that performs better than any of the weak learners taken individually. The idea is to do the boosting by doing regression on the errors of the previous iteration. As more and more iterations are run, the residuals will become smaller and smaller. The number of iterations, designated by M, can be delegated as a hyperparameter. For large values of M, there is a danger of overfitting for the model, which would not give us the best idea of the underlying function. Another hyperparameter is η, the learning rate, which is multiplied to each of the models on the residuals. Its value dictates how fast the model will update. A low η value serves the same function as a low M value: lowering the risk of overfitting. The final model will the sum of all M models, with the effects of the later models being significantly smaller than the effects of the first few. Assuming η=1, the final model is given by F<sub>M</sub>(x)=f<sub>1</sub>(x)+f<sub>2</sub>(x)+...+f<sub>M</sub>(x). We can also define each model recursively as F<sub>m</sub>(x)=F<sub>m-1</sub>(x)+f<sub>m</sub>(x) 
# Gradient Boosting with Mean Squared Error (L2 loss)
Now let's look at a particular example for a popular loss function: mean squared error. First we need to create a crude F<sub>0</sub>(x), which we will let be the mean of the target y for every value of x (a horizontal line). We will then train a regression tree Δ<sub>m</sub>, on the residual vector y-F<sub>m-1</sub>(x). We then update the model: F<sub>m</sub>(x)=F<sub>m-1</sub>(x)+ηΔ<sub>m</sub>(x). We repeat this procedure M times. 
# Gradient Boosting with Absolute Error (L1 loss)
The above model could be problematic when the data has outliers, because the squaring of the residuals will create large values for the mean squared error. This can be solved by using a different loss function, absolute error, which simply takes the absolute value of the residual. This model deviates from the above in that the F<sub>0</sub>(x) is now the median instead of the mean (because if we limit ourselves to a horizontal line, the median minimizes the absolute error while the mean minimizes the mean squared error). Another difference is that we will instead train our regression tree on the sign vector, which is the residual vector replaced by -1 for negative values, 1 for positive values, and 0 for zero values, elementwise. The result is then weighted by the absolute value of the average residual so an appreciable result occurs. But otherwise, the model acts the same and our update step is still given by F<sub>m</sub>(x)=F<sub>m-1</sub>(x)+ηΔ<sub>m</sub>(x).
# Gradient Boosting in General (Gradient Descent!)
Finally we will examine the general case: gradient boosting for any loss function. F<sub>0</sub>(x) will be the horizontal line that minimizes the loss function. We will train the regression tree on ∇<sub>F<sub>m-1</sub>(x)</sub>L(y, F<sub>m-1</sub>(x)), the gradient of the loss function with respect to the previous iteration. Once again we have F<sub>m</sub>(x)=F<sub>m-1</sub>(x)+ηΔ<sub>m</sub>(x) as our update step. In truth, the gradient of the mean squared error loss function is a constant multiple times the residual vector, and the gradient of the absolute error loss function is the sign vector of the residual vector. Therefore, the two above examples are just specific examples of the general case for gradient boosting, which involves gradient descent. 
